<!doctype html><html lang=zh-tw dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="zh-tw"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=theme-color><title>多頭注意力機制 (Multi-Head Attention) 核心筆記 &#183; Wentong's Blog</title><meta name=title content="多頭注意力機制 (Multi-Head Attention) 核心筆記 &#183; Wentong's Blog"><meta name=description content="本文深入探討多頭注意力機制的核心原理與實作細節。"><meta name=keywords content="多頭注意力機制,Transformer,學習筆記,"><meta name=robots content="index, follow"><link rel=canonical href=https://weihua0816.github.io/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/><link type=text/css rel=stylesheet href=/css/main.bundle.min.d7672814413dccdcf76c504d9b0ebb8d0f964dbe6b30a3d4b25b15809a94c2eb732d99898aded2009aacdd1fa8b17cea1369fd81560790cd355aee00bbd142fd.css integrity="sha512-12coFEE9zNz3bFBNmw67jQ+WTb5rMKPUslsVgJqUwutzLZmJit7SAJqs3R+osXzqE2n9gVYHkM01Wu4Au9FC/Q=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.6dd027da84ed605a7d30fc29766d12b001b9194125833e3ece909e4180de2b22cce999860388eb66b49811d1e47049ce8f0c0eacef8253be9e1653200a7dfc62.js integrity="sha512-bdAn2oTtYFp9MPwpdm0SsAG5GUElgz4+zpCeQYDeKyLM6ZmGA4jrZrSYEdHkcEnOjwwOrO+CU76eFlMgCn38Yg==" data-copy=複製 data-copied=已複製></script><script src=/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7+kfJ6kKCJxQGC+8wm+Bz9JucDjDTGNew=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://weihua0816.github.io/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/"><meta property="og:site_name" content="Wentong's Blog"><meta property="og:title" content="多頭注意力機制 (Multi-Head Attention) 核心筆記"><meta property="og:description" content="本文深入探討多頭注意力機制的核心原理與實作細節。"><meta property="og:locale" content="zh_tw"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-25T00:00:00+00:00"><meta property="article:tag" content="多頭注意力機制"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="學習筆記"><meta property="og:image" content="https://weihua0816.github.io/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/feature.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://weihua0816.github.io/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/feature.jpg"><meta name=twitter:title content="多頭注意力機制 (Multi-Head Attention) 核心筆記"><meta name=twitter:description content="本文深入探討多頭注意力機制的核心原理與實作細節。"><meta name=twitter:image content="https://weihua0816.github.io/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/feature.jpg"><meta property="og:image" content="https://weihua0816.github.io/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/feature.jpg"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"","name":"多頭注意力機制 (Multi-Head Attention) 核心筆記","headline":"多頭注意力機制 (Multi-Head Attention) 核心筆記","abstract":"本文深入探討多頭注意力機制的核心原理與實作細節。","inLanguage":"zh-tw","url":"https:\/\/weihua0816.github.io\/posts\/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98\/","author":{"@type":"Person","name":"Wentong"},"copyrightYear":"2025","dateCreated":"2025-08-25T00:00:00\u002b00:00","datePublished":"2025-08-25T00:00:00\u002b00:00","dateModified":"2025-08-25T00:00:00\u002b00:00","keywords":["多頭注意力機制","Transformer","學習筆記"],"mainEntityOfPage":"true","wordCount":"198"}]</script><meta name=author content="Wentong"><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>
快轉到主要內容</a></div><div class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3 pt-[2px] pr-0 pb-[3px] pl-0"><div><a href=/ class=flex><span class=sr-only>Wentong&rsquo;s Blog</span>
<span class="logo object-scale-down object-left nozoom"><svg width="13" height="13" viewBox="0 0 13 13" fill="none"><path d="M.16007 2.98446C.172568.911483 2.23364-.524383 4.18253.182181L10.277 2.39172C12.5636 3.22072 12.9651 6.28332 10.9694 7.67364L4.8358 11.9466C2.84009 13.337.106324 11.8992.120988 9.46699L.16007 2.98446z" fill="#354665"/><path d="M1.92 1.76931v7.188c0 .128.016.220000000000001.048.276C2.008 9.28131 2.1 9.30531 2.244 9.30531H3.54V10.4453H1.776C1.192 10.4453.748 10.3453.444 10.1453.148 9.94531.0 9.59731.0 9.10131v-6.648c0-.28.04-.464.12-.552C.208 1.81331.388 1.76931.66 1.76931H1.92zm1.98 2.676H5.148c.24.0.412.0679999999999996.516.204C5.768 4.77731 5.82 4.98931 5.82 5.28531v3.672c0 .128.016.220000000000001.048.276C5.908 9.28131 6 9.30531 6.144 9.30531H7.44V10.4453H4.332C4.204 10.4453 4.1 10.4013 4.02 10.3133 3.94 10.2253 3.9 10.1093 3.9 9.96531v-5.52zm4.02-2.676H9.18c.272.0.452.044.540000000000001.132C9.808 1.98931 9.852 2.17331 9.852 2.45331v7.512C9.852 10.1093 9.808 10.2253 9.72 10.3133 9.64 10.4013 9.536 10.4453 9.408 10.4453H7.92V1.76931z" fill="#F3B05E"/></svg></span></a></div><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Wentong&rsquo;s Blog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=關於我>About</p></a><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Posts</p></a><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Projects</p></a><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><div id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=關於我>About</p></a></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Posts</p></a></li><li class=mt-1><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Projects</p></a></li></ul></div></div></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom"><img id=background-image src=/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/feature_hu_31fa63f572fec5d2.jpg alt="Background Image" class="absolute inset-0 w-full h-full object-cover"><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script type=text/javascript src=/js/background-blur.min.0ad33cb9c066f652728b2cc09c9ceaf32645544f48e8b6b38f120d86f433ed997da275a49e6151fe0176430c45376812708d84727f3f969101fb44a4345b3f34.js integrity="sha512-CtM8ucBm9lJyiyzAnJzq8yZFVE9I6LazjxINhvQz7Zl9onWknmFR/gF2QwxFN2gScI2Ecn8/lpEB+0SkNFs/NA==" data-target-id=background-blur data-image-id=background-image data-image-url=/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/feature.jpg></script><header id=single_header class="mt-5 max-w-prose"><link rel=stylesheet href=/css/progress-bar.css><div id=progress-bar></div><script src=/js/progress-bar.js></script><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">多頭注意力機制 (Multi-Head Attention) 核心筆記</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-08-25T00:00:00+00:00>2025年8月25日</time><span class="px-2 text-primary-500">&#183;</span><span title=預計閱讀時間>1 分鐘</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] mr-2" href=/categories/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">深度學習
</span></span></a><a class="relative mt-[0.5rem] mr-2" href=/tags/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">多頭注意力機制
</span></span></a><a class="relative mt-[0.5rem] mr-2" href=/tags/transformer/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Transformer
</span></span></a><a class="relative mt-[0.5rem] mr-2" href=/tags/%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">學習筆記</span></span></a></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">目錄</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#多頭注意力機制-multi-head-attention-核心筆記拆解與並行><strong>多頭注意力機制 (Multi-Head Attention) 核心筆記：拆解與並行</strong></a><ul><li><a href=#一-運作原理><strong>一、 運作原理</strong></a></li><li><a href=#二-實作步驟詳解從混雜到有序的並行><strong>二、 實作步驟詳解：從混雜到有序的並行</strong></a></li><li><a href=#三-總結><strong>三、 總結</strong></a></li></ul></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">目錄</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#多頭注意力機制-multi-head-attention-核心筆記拆解與並行><strong>多頭注意力機制 (Multi-Head Attention) 核心筆記：拆解與並行</strong></a><ul><li><a href=#一-運作原理><strong>一、 運作原理</strong></a></li><li><a href=#二-實作步驟詳解從混雜到有序的並行><strong>二、 實作步驟詳解：從混雜到有序的並行</strong></a></li><li><a href=#三-總結><strong>三、 總結</strong></a></li></ul></li></ul></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><h3 class="relative group"><strong>多頭注意力機制 (Multi-Head Attention) 核心筆記：拆解與並行</strong><div id=多頭注意力機制-multi-head-attention-核心筆記拆解與並行 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100 select-none"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#%e5%a4%9a%e9%a0%ad%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a9%9f%e5%88%b6-multi-head-attention-%e6%a0%b8%e5%bf%83%e7%ad%86%e8%a8%98%e6%8b%86%e8%a7%a3%e8%88%87%e4%b8%a6%e8%a1%8c aria-label=定位點>#</a></span></h3><h4 class="relative group"><strong>一、 運作原理</strong><div id=一-運作原理 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100 select-none"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#%e4%b8%80-%e9%81%8b%e4%bd%9c%e5%8e%9f%e7%90%86 aria-label=定位點>#</a></span></h4><p>多頭注意力的關鍵在於<strong>轉置 (Transpose)</strong> 操作。<code>transpose(1, 2)</code> 讓張量形狀從 <code>(b, tokens, heads, dim)</code> 變為 <code>(b, heads, tokens, dim)</code>。這個操作的目的是：<strong>在邏輯上，將一個 token 的嵌入向量資訊分派給不同的注意力頭；在計算上，則將所有頭的運算集合起來，在張量層面進行並行處理，以高效地捕捉輸入序列中不同子空間的特徵關係。</strong></p><hr><h4 class="relative group"><strong>二、 實作步驟詳解：從混雜到有序的並行</strong><div id=二-實作步驟詳解從混雜到有序的並行 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100 select-none"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#%e4%ba%8c-%e5%af%a6%e4%bd%9c%e6%ad%a5%e9%a9%9f%e8%a9%b3%e8%a7%a3%e5%be%9e%e6%b7%b7%e9%9b%9c%e5%88%b0%e6%9c%89%e5%ba%8f%e7%9a%84%e4%b8%a6%e8%a1%8c aria-label=定位點>#</a></span></h4><p>「多頭」並不是真的有 num_heads 個獨立的 for 迴圈在跑。它的「多」是體現在<strong>張量的維度</strong>上，並透過高效的矩陣運算來實現<strong>並行處理 (Parallel Processing)</strong>。</p><p><strong>1. 準備階段：生成潛在資訊 (Projection with <code>nn.Linear</code>)</strong></p><ul><li><strong>程式碼</strong>: <code>keys = self.W_key(x)</code></li><li><strong>輸入形狀</strong>: <code>(b, num_tokens, d_in)</code></li><li><strong>輸出形狀</strong>: <code>(b, num_tokens, d_out)</code></li><li><strong>目的</strong>: 將每個輸入 token 的 <code>d_in</code> 維向量，投影到一個更高維（或相同維度）的 <code>d_out</code> 空間。可以想像 <code>d_out</code> 這個長向量中，已經<strong>混雜地包含了未來所有注意力頭所需要的全部資訊</strong>。例如，如果 <code>d_out=512</code>，這 512 個維度裡可能同時包含了語法、語意、位置等多方面的潛在特徵。</li></ul><p><strong>2. 邏輯分組：定義「頭」的邊界 (Reshape with <code>.view</code>)</strong></p><ul><li><strong>程式碼</strong>: <code>keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)</code></li><li><strong>形狀變化</strong>: <code>(b, num_tokens, d_out)</code> -> <code>(b, num_tokens, num_heads, head_dim)</code></li><li><strong>目的</strong>: 這是<strong>第一次在邏輯上體現「多頭」</strong> 的地方。此操作不改變數據本身，只改變解讀它的方式。</li><li><strong>比喻</strong>: 就像我們宣告，一條長度為 512 的繩子 (<code>d_out</code>)，現在起要被視為 8 (<code>num_heads</code>) 段長度為 64 (<code>head_dim</code>) 的短繩拼接而成。至此，對於<strong>每一個 token</strong>，我們都定義了它在 8 個不同「頭」（或稱子空間）中的獨立表徵。</li></ul><p><strong>3. 組織隊形：為並行計算而轉置 (Restructure with <code>.transpose</code>)</strong></p><ul><li><strong>程式碼</strong>: <code>keys = keys.transpose(1, 2)</code></li><li><strong>形狀變化</strong>: <code>(b, num_tokens, num_heads, head_dim)</code> -> <code>(b, num_heads, num_tokens, head_dim)</code></li><li><strong>目的</strong>: <strong>這是實現高效並行計算的關鍵</strong>。它重新組織了數據的排列順序。<ul><li><strong>轉置前</strong>: 資料以 &ldquo;Token&rdquo; 為主進行組織。先看第 1 個 token 在 8 個頭裡的樣子，再看第 2 個 token 在 8 個頭裡的樣子&mldr;</li><li><strong>轉置後</strong>: 資料以 &ldquo;<strong>Head</strong>&rdquo; 為主進行組織。先看第 1 個頭對<strong>所有 token</strong> 的樣子，再看第 2 個頭對<strong>所有 token</strong> 的樣子&mldr;</li></ul></li><li><strong>為何如此重要？</strong><ul><li>後續的矩陣乘法 <code>queries @ keys.transpose(2, 3)</code> 會在最後兩個維度上運算。</li><li>經過 <code>transpose(1, 2)</code> 後，<code>num_heads</code> 維度被推到了前面，與 <code>b</code> (批次大小) 一樣，被 PyTorch 當作<strong>批次維度 (Batch Dimension)</strong> 來處理。</li><li>這使得一個簡單的矩陣乘法指令，就能<strong>同時完成所有 <code>num_heads</code> 個頭部的注意力計算</strong>，無需使用 <code>for</code> 迴圈，從而極大地發揮了 GPU 的並行計算優勢。</li></ul></li></ul><hr><h4 class="relative group"><strong>三、 總結</strong><div id=三-總結 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100 select-none"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href=#%e4%b8%89-%e7%b8%bd%e7%b5%90 aria-label=定位點>#</a></span></h4><p>多頭注意力機制是一個「先整合，再拆分，後重組」的過程：</p><ol><li><strong>整合 (<code>nn.Linear</code>)</strong>: 將輸入 token 投影到一個富含資訊的空間。</li><li><strong>拆分 (<code>.view</code> + <code>.transpose</code>)</strong>: 在邏輯上將資訊拆成多份給不同的「頭」，並在記憶體佈局上重組它們，使其適合並行處理。</li><li><strong>重組 (<code>.view</code> + <code>out_proj</code>)</strong>: 將所有頭計算出的上下文向量拼接起來，並透過一個最終的線性層進行資訊融合，得出最終輸出。</li></ol></div><h2 class="mt-8 text-2xl font-extrabold mb-10">相關文章</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><div class="group-hover-card group relative min-h-full min-w-full overflow-hidden rounded border border-2 border-neutral-200 shadow-2xl dark:border-neutral-700"><a href=/posts/%E6%8F%AD%E7%A7%98-llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A8%93%E7%B7%B4%E9%81%8E%E7%A8%8B%E4%B8%80%E5%A0%B4%E7%B2%BE%E5%AF%86%E7%9A%84%E6%A4%8D%E7%89%A9%E6%A0%BD%E5%9F%B9%E4%B9%8B%E6%97%85/ class="absolute inset-0" aria-label="揭秘 LLM 大型語言模型的訓練過程：一場精密的植物栽培之旅"></a><div class="thumbnail_card_related nozoom w-full" style=background-image:url(/posts/%E6%8F%AD%E7%A7%98-llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A8%93%E7%B7%B4%E9%81%8E%E7%A8%8B%E4%B8%80%E5%A0%B4%E7%B2%BE%E5%AF%86%E7%9A%84%E6%A4%8D%E7%89%A9%E6%A0%BD%E5%9F%B9%E4%B9%8B%E6%97%85/feature_hu_33d30120635b0726.jpg)></div><div class="px-6 py-4"><div class="group-hover-card-title decoration-primary-500 dark:text-neutral text-xl font-bold text-neutral-800 group-hover:underline group-hover:underline-offset-2">揭秘 LLM 大型語言模型的訓練過程：一場精密的植物栽培之旅</div><div class="group-hover-cancel text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-08-25T00:00:00+00:00>2025年8月25日</time><span class="px-2 text-primary-500">&#183;</span><span title=預計閱讀時間>1 分鐘</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] mr-2" href=/categories/ai-%E6%A6%82%E8%AB%96/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI 概論
</span></span></a><a class="relative mt-[0.5rem] mr-2" href=/tags/transformer/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Transformer
</span></span></a><a class="relative mt-[0.5rem] mr-2" href=/tags/ai/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI
</span></span></a><a class="relative mt-[0.5rem] mr-2" href=/tags/%E7%9F%A5%E8%AD%98%E7%A7%91%E6%99%AE/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">知識科普</span></span></a></div></div><div class="prose dark:prose-invert py-1">本文簡潔的介紹了 LLM 大型語言模型的訓練過程，並以植物栽培之旅為比喻，讓讀者更容易理解。</div></div><div class="px-6 pt-4 pb-2"></div></div></section></div><script type=text/javascript src=/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA==" data-oid="views_posts/多頭注意力機制 (Multi-Head Attention) 核心筆記/index.md" data-oid-likes="likes_posts/多頭注意力機制 (Multi-Head Attention) 核心筆記/index.md"></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/%E6%8F%AD%E7%A7%98-llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A8%93%E7%B7%B4%E9%81%8E%E7%A8%8B%E4%B8%80%E5%A0%B4%E7%B2%BE%E5%AF%86%E7%9A%84%E6%A4%8D%E7%89%A9%E6%A0%BD%E5%9F%B9%E4%B9%8B%E6%97%85/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">揭秘 LLM 大型語言模型的訓練過程：一場精密的植物栽培之旅</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-08-25T00:00:00+00:00>2025年8月25日</time>
</span></span></a></span><span></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0 z-10"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label=捲動到頁頂 title=捲動到頁頂>&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex list-none flex-col sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/categories/ title=Categories>Categories</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Wentong</p><p class="text-xs text-neutral-500 dark:text-neutral-400">以 <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a> 製作</p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer></div></body></html>