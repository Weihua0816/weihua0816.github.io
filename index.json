[{"content":"","date":"2025年8月29日","externalUrl":null,"permalink":"/","section":"Wentong's Blog","summary":"","title":"Wentong's Blog","type":"page"},{"content":" 自我介紹，對! # 介紹完畢 # ","date":"2025年8月29日","externalUrl":null,"permalink":"/about/","section":"Wentong's Blog","summary":"還不知道寫什麼","title":"關於我","type":"page"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/categories/ai-%E6%A6%82%E8%AB%96/","section":"Categories","summary":"","title":"AI 概論","type":"categories"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/tags/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6/","section":"Tags","summary":"","title":"多頭注意力機制","type":"tags"},{"content":" 多頭注意力機制 (Multi-Head Attention) 核心筆記：拆解與並行 # 一、 運作原理 # 多頭注意力的關鍵在於轉置 (Transpose) 操作。transpose(1, 2) 讓張量形狀從 (b, tokens, heads, dim) 變為 (b, heads, tokens, dim)。這個操作的目的是：在邏輯上，將一個 token 的嵌入向量資訊分派給不同的注意力頭；在計算上，則將所有頭的運算集合起來，在張量層面進行並行處理，以高效地捕捉輸入序列中不同子空間的特徵關係。\n二、 實作步驟詳解：從混雜到有序的並行 # 「多頭」並不是真的有 num_heads 個獨立的 for 迴圈在跑。它的「多」是體現在張量的維度上，並透過高效的矩陣運算來實現並行處理 (Parallel Processing)。\n1. 準備階段：生成潛在資訊 (Projection with nn.Linear)\n程式碼: keys = self.W_key(x) 輸入形狀: (b, num_tokens, d_in) 輸出形狀: (b, num_tokens, d_out) 目的: 將每個輸入 token 的 d_in 維向量，投影到一個更高維（或相同維度）的 d_out 空間。可以想像 d_out 這個長向量中，已經混雜地包含了未來所有注意力頭所需要的全部資訊。例如，如果 d_out=512，這 512 個維度裡可能同時包含了語法、語意、位置等多方面的潛在特徵。 2. 邏輯分組：定義「頭」的邊界 (Reshape with .view)\n程式碼: keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) 形狀變化: (b, num_tokens, d_out) -\u0026gt; (b, num_tokens, num_heads, head_dim) 目的: 這是第一次在邏輯上體現「多頭」 的地方。此操作不改變數據本身，只改變解讀它的方式。 比喻: 就像我們宣告，一條長度為 512 的繩子 (d_out)，現在起要被視為 8 (num_heads) 段長度為 64 (head_dim) 的短繩拼接而成。至此，對於每一個 token，我們都定義了它在 8 個不同「頭」（或稱子空間）中的獨立表徵。 3. 組織隊形：為並行計算而轉置 (Restructure with .transpose)\n程式碼: keys = keys.transpose(1, 2) 形狀變化: (b, num_tokens, num_heads, head_dim) -\u0026gt; (b, num_heads, num_tokens, head_dim) 目的: 這是實現高效並行計算的關鍵。它重新組織了數據的排列順序。 轉置前: 資料以 \u0026ldquo;Token\u0026rdquo; 為主進行組織。先看第 1 個 token 在 8 個頭裡的樣子，再看第 2 個 token 在 8 個頭裡的樣子\u0026hellip; 轉置後: 資料以 \u0026ldquo;Head\u0026rdquo; 為主進行組織。先看第 1 個頭對所有 token 的樣子，再看第 2 個頭對所有 token 的樣子\u0026hellip; 為何如此重要？ 後續的矩陣乘法 queries @ keys.transpose(2, 3) 會在最後兩個維度上運算。 經過 transpose(1, 2) 後，num_heads 維度被推到了前面，與 b (批次大小) 一樣，被 PyTorch 當作批次維度 (Batch Dimension) 來處理。 這使得一個簡單的矩陣乘法指令，就能同時完成所有 num_heads 個頭部的注意力計算，無需使用 for 迴圈，從而極大地發揮了 GPU 的並行計算優勢。 三、 總結 # 多頭注意力機制是一個「先整合，再拆分，後重組」的過程：\n整合 (nn.Linear): 將輸入 token 投影到一個富含資訊的空間。 拆分 (.view + .transpose): 在邏輯上將資訊拆成多份給不同的「頭」，並在記憶體佈局上重組它們，使其適合並行處理。 重組 (.view + out_proj): 將所有頭計算出的上下文向量拼接起來，並透過一個最終的線性層進行資訊融合，得出最終輸出。 ","date":"2025年8月25日","externalUrl":null,"permalink":"/posts/%E5%A4%9A%E9%A0%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-multi-head-attention-%E6%A0%B8%E5%BF%83%E7%AD%86%E8%A8%98/","section":"","summary":"本文深入探討多頭注意力機制的核心原理與實作細節。","title":"多頭注意力機制 (Multi-Head Attention) 核心筆記","type":"posts"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/tags/%E7%9F%A5%E8%AD%98%E7%A7%91%E6%99%AE/","section":"Tags","summary":"","title":"知識科普","type":"tags"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/","section":"Categories","summary":"","title":"深度學習","type":"categories"},{"content":" 揭秘 LLM 大型語言模型的訓練過程：一場精密的植物栽培之旅 # 大型語言模型（LLM）的訓練過程，如同培育一株從幼苗到參天大樹的精密植物栽培之旅。這個過程融合了數據的滋養、架構的選擇以及細緻入微的照護，最終使其能夠處理和生成複雜的人類語言。\n這場栽培之旅始於「土壤的選擇」——這就是模型的訓練資料 (Training Data)。如同肥沃的土壤是植物生長的基礎，大量且高品質、多樣化的訓練資料是 LLM 學習知識的根基，其豐富性直接決定了模型能吸收多少養分。接著，我們需要選擇合適的「種子」——這便是模型架構 (Model Architecture)。當前主流的 LLM「種子」是廣泛採用的 Transformer 架構，它的「基因」設計，特別是其核心的注意力機制，定義了模型學習和處理資訊的基本方式。\n當種子播下後，便進入了精心的「照護階段」，這正是我們作為「園丁」設定各種**超參數 (Hyperparameters)**的過程。這些超參數是指導模型學習的「栽培策略」。例如，「訓練週期 (Epochs)」就好比讓植物將土壤中的所有養分（整個訓練資料集）徹底吸收消化一遍的完整過程，多次重複則能讓模型更充分地鞏固所學。而每次澆水或施肥的「量」則類似於「訓練批次 (Batch Size)」，它決定了模型每次吸收資料和調整內部結構的單位大小。同時，精準的「修剪枝葉」則是「學習率 (Learning Rate)」，這個「力度」控制著模型調整其內部參數的速度和幅度，過快可能導致「剪過頭」，過慢則效率不彰。\n在栽培過程中，我們還需要一個重要的參考——「植物健康度量表」，這就是損失函數 (Loss Function)。它不斷衡量模型當前的預測結果與真實答案之間的「錯誤程度」或「不健康狀態」，指引著園丁（我們）調整栽培策略，目標是讓這個「不健康度」越來越小。而執行這些修剪、澆水、施肥動作的「栽培手法或工具」便是優化器 (Optimizer)，如 Adam 或 SGD。它們根據損失函數的指示和學習率的設定，以特定的演算法來調整模型的內部結構，確保其健康成長。\n最終，深入到植物的內部，其細胞、組織和脈絡的精密運作，對應著模型內部數量龐大（數十億甚至數千億）的模型權重 (Model Weights)。這些權重在訓練過程中會根據外部的「陽光、水、肥」和內部的「基因」自行調整、適應和優化。這些是模型在訓練過程中自動形成的內部參數，其複雜性和動態性使得我們無法一一手動干預。\n總的來說，LLM 的訓練過程是一場精密的系統工程，透過訓練資料的餵養、模型架構的設計、超參數的精心調控、損失函數的引導以及優化器的執行，模型不斷調整其龐大的內部權重，最終成長為能夠理解和生成複雜語言的強大智慧體。\nLLM 大型語言模型訓練過程：摘要重點 # 選擇土壤 (訓練資料 - Training Data)： 如同栽種前需挑選肥沃、適合的土壤，LLM 的訓練首先需要大量、高品質且多樣化的「訓練資料」。這些資料是模型吸收知識的養分來源，決定了模型能學到什麼。\n挑選種子 (模型架構 - Model Architecture)： 接著，我們選擇適合的「種子」進行種植。目前主流的 LLM「種子」是 Transformer 架構，其內部的設計（如注意力機制）決定了模型學習和處理資訊的基本方式。\n園丁的栽培策略 (超參數 - Hyperparameters)： 植物的茁壯成長需要園丁精心照護，這些人為手動設定的「栽培策略」就是超參數，它們指導著模型如何學習：\n訓練週期 (Epochs)： 想像植物要將土壤中的全部養分（整個訓練資料集）徹底吸收消化一遍，這就是一個「訓練週期」。多個週期意味著模型反覆地從相同的資料中學習，以鞏固知識。 訓練批次 (Batch Size)： 這好比每次澆水或施肥時，「一次性供給給多少株植物來進行養分吸收和生長調整的『單位量』」。它決定了模型多久「總結一次」學到的東西並調整內部結構。 學習率 (Learning Rate)： 這是「修剪枝葉的『力度』或『幅度』」。學習率高，調整快但可能過度；學習率低，調整慢但更精細。 植物健康度量表 (損失函數 - Loss Function)： 為了知道植物是否健康成長，我們需要一個「健康度量表」。在 LLM 訓練中，「損失函數」就是這個度量表，它衡量模型預測結果與真實答案之間的「錯誤程度」。目標是透過訓練使這個錯誤值越來越小。\n園丁的栽培手法/工具 (優化器 - Optimizer)： 有了度量表，園丁需要利用不同的「栽培手法或工具」（如 Adam、SGD 等優化器）來執行修剪、澆水、施肥等動作。優化器根據損失函數的指示和學習率的設定，以特定的演算法來調整模型的內部結構。\n植物細胞的自我調整 (模型權重 - Model Weights)： 最後，縮小到植物的內部，其細胞、組織、脈絡（數十億甚至數千億的「模型權重」）會根據外部的陽光、水、肥以及內部基因的指令，進行龐大而精微的自我調整和優化。這些是模型在訓練過程中自動形成的內部參數，我們無法一一手動干預。\n整個過程是模型在訓練資料、超參數、損失函數和優化器的協同作用下，不斷學習、調整和優化其內部權重，最終成長為一個強大且能執行複雜任務的語言模型。\n","date":"2025年8月25日","externalUrl":null,"permalink":"/posts/%E6%8F%AD%E7%A7%98-llm-%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A8%93%E7%B7%B4%E9%81%8E%E7%A8%8B%E4%B8%80%E5%A0%B4%E7%B2%BE%E5%AF%86%E7%9A%84%E6%A4%8D%E7%89%A9%E6%A0%BD%E5%9F%B9%E4%B9%8B%E6%97%85/","section":"","summary":"本文簡潔的介紹了 LLM 大型語言模型的訓練過程，並以植物栽培之旅為比喻，讓讀者更容易理解。","title":"揭秘 LLM 大型語言模型的訓練過程：一場精密的植物栽培之旅","type":"posts"},{"content":"","date":"2025年8月25日","externalUrl":null,"permalink":"/tags/%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98/","section":"Tags","summary":"","title":"學習筆記","type":"tags"},{"content":"","externalUrl":null,"permalink":"/projects/","section":"","summary":"","title":"","type":"projects"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]